#!/usr/bin/env python

"""
Handling the computer vision for flipping pancakes

SUBSCRIPTIONS:
    + /camera/aligned_depth_to_color/image_raw (Image) ~ the depth image from the RealSense camera, aligned with the color
    + /camera/color/image_raw (Image) ~ the color image from the RealSense
    + /camera/aligned_depth_to_color/camera_info (CameraInfo) ~ the RealSense camera intrinsics, used in converting pixels to XYZ coordinates

PARAMETERS:
    + pancaked ~ a boolean flag for when the pour sequence has been completed
    + flip_time ~ a boolean flag for when the vision code determines it's time to flip
    + flipped ~ a boolean flag for when the flip sequence has been completed

BROADCASTERS:
    + flip_location ~ static coordinates in world frame for where the pancake is at the time of flipping
    + lift_location ~ static coordinates in world frame for where the pancake is at the time of flipping

"""

from numpy.core.defchararray import lower
import rospy
from cv_bridge import CvBridge, CvBridgeError
from sensor_msgs.msg import Image, CompressedImage, CameraInfo
import numpy as np
import cv2
import pyrealsense2 as rs
import tf2_ros
import tf2_geometry_msgs
from geometry_msgs.msg import TransformStamped
from tf import transformations, TransformListener


class ImageProcessing:

    def __init__(self):
        rospy.init_node('vision_test_node',anonymous=True, log_level=rospy.DEBUG)

        rospy.sleep(2)
        rospy.Subscriber("/camera/aligned_depth_to_color/image_raw", Image, callback=self.convert_depth_image, queue_size=1)
        rospy.Subscriber("/camera/color/image_raw", Image, callback=self.convert_color_image, queue_size=1)
        rospy.Subscriber("/camera/aligned_depth_to_color/camera_info", CameraInfo, callback=self.get_intrinsics, queue_size=1)
        rospy.set_param("/pancaked", False)
        rospy.set_param("/flipped", False)
        rospy.set_param("/flip_time", False)

        self.point_counter = 0
        self.point_matrix = np.zeros((4,2),np.int)
        self.first_frame_after_pour = True
        self.flag2 = False
        
    def show_video(self, name, ros_image):
        """ Shows a video feed for debugging

        Displays a video window of the specified image

        Args:
            name (string) : the name of the window to be created
            ros_image (ndarray) : the image to be shown
        Returns:
            None
        """
        cv2.imshow(name,ros_image)
        cv2.waitKey(3)

        return

    def convert_depth_image(self,ros_image):
        """ Converts the RealSense Image message for the depth channel
        
        Uses CvBridge to convert the Image message into a ndarray suitable for OpenCV

        Args:
            ros_image (Image) : The Image messsage published by the RealSense to the /camera/aligned_depth_to_color/image_raw topic
        Returns:
            None
        """
        bridge = CvBridge()
        # Use cv_bridge() to convert the ROS image to OpenCV format
        try:
            self.depth_image = bridge.imgmsg_to_cv2(ros_image)
            depth_array = np.array(self.depth_image, dtype=np.float32)
            #self.show_video(depth_image)

        except CvBridgeError:
            print(CvBridgeError())
            rospy.logerr("bad")
        
        return

    def convert_color_image(self,ros_image):
        """ Converts the RealSense Image message for the color channel

        Additionally calls functions for per-color-frame computer vision for pancake detection using OpenCV

        Args:
            ros_image (Image) : The Image messsage published by the RealSense to the /camera/color/image_raw topic
        Returns:
            None
        """

        bridge = CvBridge()
        try:
            self.bgr_image = bridge.imgmsg_to_cv2(ros_image)
            self.hsv_image = cv2.cvtColor(self.bgr_image, cv2.COLOR_BGR2HSV)
            self.rgb_image = cv2.cvtColor(self.bgr_image, cv2.COLOR_BGR2RGB)
            self.grayscale_image = cv2.cvtColor(self.bgr_image, cv2.COLOR_BGR2GRAY)           

            self.find_bottle_location()
            
            self.pancake_flag = rospy.get_param("/pancaked")
            if self.pancake_flag:
                self.find_pancake_location()

            self.flip_flag = rospy.get_param("/flipped")
            if self.flip_flag:
                self.find_lift_location()
                rospy.set_param("/flipped", False)

            self.fliptime = rospy.get_param("/flip_time")
            if self.fliptime:
                self.rgb_image = cv2.circle(self.rgb_image, (self.cx,self.cy), 3, (255,255,0), 3)

            self.show_video("test",self.rgb_image)

        except CvBridgeError:
            print(CvBridgeError())
            rospy.logerr("bad")
        
        return

    def get_intrinsics(self, camera_info):
        """ Gets the intrinsics from the /camera/aligned_depth_to_color/camera_info topic
        Intrinsics are used for finding the XYZ position of each pixel in the image

        Args:
            camera_info (CameraInfo) : The message published by the Realsense to the /camera/aligned_depth_to_color/camera_info topic
        Returns:
            None
        """
        self.intr = rs.intrinsics()
        self.intr.width = camera_info.width
        self.intr.height = camera_info.height
        self.intr.ppx = camera_info.K[2]
        self.intr.ppy = camera_info.K[5]
        self.intr.fx = camera_info.K[0]
        self.intr.fy = camera_info.K[4]
        self.intr.model = rs.distortion.none
        self.intr.coeffs = [0.0, 0.0, 0.0, 0.0, 0.0]

        return

    def find_bottle_location(self):  
        """ Finds the location of the bottle

        This function is currently deprecated as AprilTags are used for bottle detection instead

        Args:
            None
        Returns:
            None
        """          

        red_low = np.array([80,10,10])
        red_up = np.array([255,255,255])

        self.redmask = cv2.inRange(self.hsv_image,red_low,red_up)

        self.masked_image = cv2.bitwise_and(self.hsv_image,self.hsv_image,mask=self.redmask)

        self.masked_image_gray = cv2.cvtColor(self.masked_image, cv2.COLOR_RGB2GRAY)
        
        ret, thresh = cv2.threshold(self.masked_image_gray, 130,255, cv2.THRESH_BINARY)

        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

        areas = [cv2.contourArea(c) for c in contours]
        max_index = np.argmax(areas)
        cnt=contours[max_index]

        x,y,w,h = cv2.boundingRect(cnt)
        cv2.rectangle(self.rgb_image,(x,y),(x+w,y+h),(0,255,0),2)

        M = cv2.moments(cnt)
        self.cx_bottle = int(M['m10']/M['m00'])
        self.cy_bottle = int(M['m01']/M['m00'])

        cv2.circle(self.rgb_image, (self.cx_bottle,self.cy_bottle), 4, (255,0,0), 3)

        result = self.get_xyz_from_image(self.depth_image, self.cx_bottle, self.cy_bottle)

        return

    def find_pancake_location(self):
        """ Finds the location of the pancake before flipping

        Uses contour detection to determine an appropriate time to flip the pancake
        Broadcasts the pancake position to /tf as a static transform /flip_location

        Args:
            None
        Returns:
            None
        """
        self.pancake_pos = rospy.get_param("/saved_pancake_pose")
        
        # Make a black mask
        black_mask = np.zeros((720, 1280), np.uint8)

        # Draw a circle in the mask and RGB image
        cv2.circle(black_mask, (445,471), 180 ,255, -1)
        self.rgb_image = cv2.circle(self.rgb_image, (445,471), 180, (255,0,0), 3)

        # Apply thresholding to reduce the chance of finding something other than the pancake
        thresh,dst = cv2.threshold(self.grayscale_image, 100, 255, cv2.THRESH_BINARY)

        # Mask the images
        masked_img = cv2.bitwise_and(black_mask, dst)
        # self.show_video("pancake",masked_img)
        
        # Apply Canny edge detection and contour finding to determine+count the contours
        canny_pancake = cv2.Canny(masked_img,50,150, 5, L2gradient = True)

        contours, heir = cv2.findContours(canny_pancake, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

        num_contours = len(contours)
        # rospy.logdebug(num_contours)
        # self.show_video("canny", canny_pancake)

        self.now = rospy.Time.now()

        if self.first_frame_after_pour == False:

            # The condition for when the detection times out, changes the flags anyways so the pancake doesn't burn
            if rospy.Time.now() > self.starttime + self.cooktime:
                rospy.logdebug("TIME TO FLIP, TIMED OUT")

                hull_list = []

                # Finds the second largest (by area) convex hull uses that as the pancake location to broadcast
                # The largest contour is the circle of the mask itself
                for i in range(len(contours)):
                    hull = cv2.convexHull(contours[i])
                    hull_list.append(hull)

                self.contours_by_area_flip = sorted(contours, key=cv2.contourArea, reverse=True)
                self.hulls_by_area_flip = sorted(hull_list, key=cv2.contourArea, reverse=True)

                M = cv2.moments(self.hulls_by_area_flip[1])
                self.cx = int(M['m10']/M['m00'])
                self.cy = int(M['m01']/M['m00'])

                self.rgb_image = cv2.circle(self.rgb_image, (self.cx,self.cy), 3, (255,255,0), 3)

                self.flip_coords = self.get_xyz_from_image(self.depth_image, self.cx, self.cy)

                rospy.set_param("/flip_location", self.flip_coords)
                rospy.logerr(self.flip_coords)
                rospy.set_param("/flip_time", True)
                rospy.set_param("/pancaked", False)

                ###########################

                static_broadcaster = tf2_ros.StaticTransformBroadcaster()
                flip_tf = TransformStamped()
                flip_tf.header.stamp = rospy.Time.now()
                flip_tf.header.frame_id = "camera_color_optical_frame"
                flip_tf.child_frame_id = "flip_location"
                flip_tf.transform.translation.x = self.flip_coords[0]/1000
                flip_tf.transform.translation.y = self.flip_coords[1]/1000
                flip_tf.transform.translation.z = self.flip_coords[2]/1000

                q2 = transformations.quaternion_from_euler(0, 0, 0)
                rospy.logdebug(q2)
                flip_tf.transform.rotation.w = 1
                static_broadcaster.sendTransform(flip_tf)
                rospy.logdebug("broadcasting flip")

                ##############################

        # Notes the first time this runs (immediately after pouring) and counts the original number of contours
        if self.first_frame_after_pour:
            self.starttime = rospy.Time.now()
            self.cooktime = rospy.Duration(60)
            self.orig_contour_count = num_contours
            self.first_frame_after_pour = False

        # Set a flag when the number of contours exceed a threshold
        if num_contours > self.orig_contour_count * 1.2:
            self.flag2 = True
            
            rospy.logdebug("TIME TO FLIP?")

        # After the threshold has been exceeded & contour counts go back down, flip the pancake
        if self.flag2 and num_contours < 70:
            rospy.logdebug("really oughta flip now")

            hull_list = []

            for i in range(len(contours)):
                hull = cv2.convexHull(contours[i])
                hull_list.append(hull)

            self.contours_by_area_flip = sorted(contours, key=cv2.contourArea, reverse=True)
            self.hulls_by_area_flip = sorted(hull_list, key=cv2.contourArea, reverse=True)

            M = cv2.moments(self.hulls_by_area_flip[1])
            self.cx = int(M['m10']/M['m00'])
            self.cy = int(M['m01']/M['m00'])

            self.rgb_image = cv2.circle(self.rgb_image, (self.cx,self.cy), 3, (255,255,0), 3)

            self.flip_coords = self.get_xyz_from_image(self.depth_image, self.cx, self.cy)

            rospy.set_param("/flip_location", self.flip_coords)
            rospy.logerr(self.flip_coords)
            rospy.set_param("/flip_time", True)
            rospy.set_param("/pancaked", False)

            #########################

            rospy.logdebug("testing broadcaster flip")
            static_broadcaster = tf2_ros.StaticTransformBroadcaster()
            flip_tf = TransformStamped()
            flip_tf.header.stamp = rospy.Time.now()
            flip_tf.header.frame_id = "camera_color_optical_frame"
            flip_tf.child_frame_id = "flip_location"
            flip_tf.transform.translation.x = self.flip_coords[0]/1000
            flip_tf.transform.translation.y = self.flip_coords[1]/1000
            flip_tf.transform.translation.z = self.flip_coords[2]/1000
            
            q2 = transformations.quaternion_from_euler(0, 0, 0)
            rospy.logdebug(q2)
            flip_tf.transform.rotation.w = 1
            static_broadcaster.sendTransform(flip_tf)
            rospy.logdebug("broadcasting flip")

            ##########################

    def find_lift_location(self):
        """ Finds the location of the pancake before lifting

        Uses contour detection to determine an appropriate the location of the pancake for lifting
        Broadcasts the pancake position to /tf as a static transform /lift_location

        Args:
            None
        Returns:
            None
        """
        
        rospy.logerr("finding lift location")

        black_mask = np.zeros((720, 1280), np.uint8)

        # Perform the same masking as before
        cv2.circle(black_mask, (self.cx,self.cy), 180, 255, -1)
        self.rgb_image = cv2.circle(self.rgb_image, (self.cx,self.cy), 180, (0,0,255), 3)

        thresh,dst = cv2.threshold(self.grayscale_image, 100, 255, cv2.THRESH_BINARY)

        masked_img_lift = cv2.bitwise_and(black_mask, dst)

        rospy.logdebug("drew a circle where we last saw the pancake, lifting this time. also thresholding works?")
        
        canny_pancake_lift = cv2.Canny(masked_img_lift,50,150, 5, L2gradient = True)

        self.show_video("canny?", canny_pancake_lift)


        contours, heir = cv2.findContours(canny_pancake_lift, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

        hull_list = []

        for i in range(len(contours)):
            hull = cv2.convexHull(contours[i])
            hull_list.append(hull)

        self.contours_by_area_lift = sorted(contours, key=cv2.contourArea, reverse=True)
        self.hulls_by_area_lift = sorted(hull_list, key=cv2.contourArea, reverse=True)

        M = cv2.moments(self.hulls_by_area_lift[1])
        self.cx = int(M['m10']/M['m00'])
        self.cy = int(M['m01']/M['m00'])

        self.rgb_image = cv2.circle(self.rgb_image, (self.cx,self.cy), 3, (255,0,0), 3)

        self.lift_coords = self.get_xyz_from_image(self.depth_image, self.cx, self.cy)

        rospy.set_param("/flip_location", self.lift_coords)
        rospy.logerr("updated flip location")

        rospy.logdebug("testing broadcaster lift")
        static_broadcaster = tf2_ros.StaticTransformBroadcaster()
        lift_tf = TransformStamped()
        lift_tf.header.stamp = rospy.Time.now()
        lift_tf.header.frame_id = "camera_color_optical_frame"
        lift_tf.child_frame_id = "lift_location"
        lift_tf.transform.translation.x = self.lift_coords[0]/1000
        lift_tf.transform.translation.y = self.lift_coords[1]/1000
        lift_tf.transform.translation.z = self.lift_coords[2]/1000
        #link0_atag_tf.transform.rotation = self.robot_atag_q
        #q1 = transformations.quaternion_about_axis(0, [0, 0, 1])
        q2 = transformations.quaternion_from_euler(0, 0, 0)
        rospy.logdebug(q2)
        lift_tf.transform.rotation.w = 1
        static_broadcaster.sendTransform(lift_tf)
        rospy.logdebug("broadcasting lift")



    def get_xyz_from_image(self,depth_image, x,y):
        """ Produces the XYZ values relative to the camera for a pixel

        Given x, y (pixel coordinates), finds XYZ in millimeters relative to the camera using its intrinsics

        Args:
            depth_image (ndarray) : The depth image after conversion by CvBridge
            x (int) : the desired pixel's y (row) coordinate
            y (int) : the desired pixel's x (col) coordinate
        Returns:
            [x, y, z]: the coordinates in millimeters of the pixel relative to the camera
        """
        
        result = rs.rs2_deproject_pixel_to_point(self.intr, [x,y], depth_image[y,x])
        return result 

if __name__ == '__main__':
    ImageProcessing()
    rospy.spin()